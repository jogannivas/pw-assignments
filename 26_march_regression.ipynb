{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1\n",
        "\n",
        "Simple Linear Regression:\n",
        "Simple linear regression is a statistical technique used to model the relationship between two variables: one independent variable (X) and one dependent variable (Y). It assumes a linear relationship between X and Y, meaning that the change in Y can be explained by a constant slope multiplied by the change in X. The goal of simple linear regression is to find the best-fit line that minimizes the sum of the squared differences between the predicted and actual Y values.\n",
        "\n",
        "Example of Simple Linear Regression:\n",
        "Suppose we want to analyze the relationship between the number of hours studied (X) and the exam score (Y) of a group of students. We collect data on 10 students, recording the number of hours studied and their corresponding exam scores. We can then perform a simple linear regression analysis to model the relationship between these two variables and predict the exam score based on the number of hours studied."
      ],
      "metadata": {
        "id": "0iUEJT4sl2GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2\n",
        "\n",
        "Linear regression makes several assumptions about the data in order for the model to be valid and reliable. It is important to assess these assumptions to ensure the appropriateness of linear regression for a given dataset. Here are the key assumptions of linear regression:\n",
        "\n",
        "Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear. This means that the change in the dependent variable is proportional to the change in each independent variable, and the combined effect of all variables is additive.\n",
        "\n",
        "Independence: The observations in the dataset are assumed to be independent of each other. This means that the value of one observation does not influence the value of another observation.\n",
        "\n",
        "Homoscedasticity: Homoscedasticity assumes that the variance of the errors (residuals) is constant across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the dependent variable.\n",
        "\n",
        "Normality: The residuals are assumed to be normally distributed. This assumption is important for hypothesis testing, confidence intervals, and making accurate predictions.\n",
        "\n",
        "No multicollinearity: There should be little to no multicollinearity among the independent variables. Multicollinearity occurs when there is a high correlation between independent variables, making it difficult to separate their individual effects on the dependent variable."
      ],
      "metadata": {
        "id": "Me95cFl4l2DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Q3\n",
        "\n",
        "\n",
        "In a linear regression model, the slope and intercept have specific interpretations:\n",
        "\n",
        "Intercept (β₀): The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X) are equal to zero. It is the point where the regression line intersects the y-axis.\n",
        "\n",
        "Slope (β₁, β₂, β₃, ...): The slope(s) represent(s) the change in the dependent variable (Y) associated with a one-unit change in the corresponding independent variable (X), assuming all other independent variables are held constant. The slope indicates the direction and magnitude of the relationship between the independent variable(s) and the dependent variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "5FicZGWal2Ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4\n",
        "\n",
        "Gradient descent is an optimization algorithm used in machine learning to minimize the cost function or error of a model. It is particularly useful in training models that have a large number of parameters or when it is computationally expensive to calculate the optimal solution directly.\n",
        "\n",
        "The main idea behind gradient descent is to iteratively update the model's parameters in the direction of steepest descent of the cost function. The gradient of the cost function represents the direction of the greatest rate of change, and by iteratively adjusting the parameters in the opposite direction of the gradient, the algorithm aims to reach the minimum of the cost function.\n",
        "\n",
        "Here's a simplified explanation of the gradient descent process:\n",
        "\n",
        "Initialize the model's parameters with some initial values.\n",
        "Calculate the cost function, which measures the error between the predicted and actual values.\n",
        "Compute the gradient of the cost function with respect to each parameter. This involves taking partial derivatives of the cost function with respect to each parameter.\n",
        "Update the parameters by subtracting a fraction (learning rate) of the gradient from the current parameter values. The learning rate controls the size of the steps taken in the parameter space.\n",
        "Repeat steps 2 to 4 until the algorithm converges, i.e., the cost function is minimized or reaches a predefined threshold, or a maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "iwbk7himl19u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5\n",
        "\n",
        "Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable (Y) and multiple independent variables (X₁, X₂, X₃, ..., Xₙ). It assumes a linear relationship between the dependent variable and each independent variable while holding other variables constant.\n",
        "\n",
        "The multiple linear regression model can be represented as follows:\n",
        "\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + ... + βₙXₙ + ε\n",
        "\n",
        "In this equation:\n",
        "\n",
        "Y represents the dependent variable (the variable to be predicted or explained).\n",
        "X₁, X₂, X₃, ..., Xₙ are the independent variables (also known as predictors or features).\n",
        "β₀, β₁, β₂, β₃, ..., βₙ are the coefficients (slopes) associated with each independent variable, representing the change in the dependent variable when that particular independent variable changes by one unit, assuming all other independent variables are held constant.\n",
        "ε represents the error term, which captures the variability in the dependent variable that is not explained by the independent variables.\n",
        "The main difference between multiple linear regression and simple linear regression is the number of independent variables considered. Simple linear regression involves only one independent variable, while multiple linear regression involves two or more independent variables."
      ],
      "metadata": {
        "id": "VMR1w6Yul161"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6\n",
        "\n",
        "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. This correlation can cause problems in the model, as it becomes difficult to separate the individual effects of the correlated variables on the dependent variable. Multicollinearity can lead to unstable and unreliable coefficient estimates, reduced statistical significance, and difficulties in interpreting the model.\n",
        "\n",
        "Detecting Multicollinearity:\n",
        "There are several methods to detect multicollinearity:\n",
        "\n",
        "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
        "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable, which measures how much the variance of the coefficient estimates is increased due to multicollinearity. Higher VIF values (>5 or >10) indicate multicollinearity."
      ],
      "metadata": {
        "id": "kaCX7tAGl14G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7\n",
        "\n",
        "Polynomial regression is a type of regression analysis that models the relationship between the dependent variable (Y) and the independent variable (X) as an nth-degree polynomial function. In polynomial regression, instead of assuming a linear relationship as in simple or multiple linear regression, the model allows for more complex and nonlinear relationships between the variables."
      ],
      "metadata": {
        "id": "b4lLK0NJl11V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8\n",
        "\n",
        "Advantages of Polynomial Regression compared to Linear Regression:\n",
        "\n",
        "Nonlinear Relationships: Polynomial regression can capture nonlinear relationships between the independent and dependent variables, whereas linear regression assumes a linear relationship. This allows for more flexibility in modeling complex patterns in the data.\n",
        "\n",
        "Higher Accuracy: When the underlying relationship between the variables is nonlinear, using polynomial regression can result in higher model accuracy compared to linear regression. It can fit the data more closely and provide better predictions.\n",
        "\n",
        "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
        "\n",
        "Overfitting: Polynomial regression with a high degree polynomial can be prone to overfitting. Overfitting occurs when the model fits the training data too closely, capturing noise and random fluctuations. This can lead to poor generalization to new, unseen data."
      ],
      "metadata": {
        "id": "mPqUgsxcl1yl"
      }
    }
  ]
}