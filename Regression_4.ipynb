{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1\n",
        "Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm)."
      ],
      "metadata": {
        "id": "jpUe6motwS7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2\n",
        "Advantages of LASSO regression\n",
        "The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
      ],
      "metadata": {
        "id": "BWXvxU0EwS25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3\n",
        "The coefficients can be used to understand the impact of each feature on the target variable, and also help in feature selection. In this case, we can see that some of the coefficients are zero, indicating that those features may not be important in predicting the target variable"
      ],
      "metadata": {
        "id": "9EKJ0AvewSyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4\n",
        "A tuning parameter (Î»), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean."
      ],
      "metadata": {
        "id": "uP3hQs58wSuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5\n",
        "If you can linearize the model, then yes but for an approximate solution in the LS sense since what is measured is y and not any of its possible transforms. If you model is nonlinear because of one parameter, there are things which can be done"
      ],
      "metadata": {
        "id": "CwE3AGzLwSpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6\n",
        "Ridge regression shrinks the coefficients towards zero, while Lasso regression encourages some of them to be exactly zero. These techniques can be implemented easily in Python using scikit-learn, making it accessible to a wide audience."
      ],
      "metadata": {
        "id": "e6werzPcwSjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7\n",
        "Lasso Regression\n",
        "Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity."
      ],
      "metadata": {
        "id": "hYCE0CFEwSdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8\n",
        "The best cross-validation score is obtained for the 0.4 value of lambda. This is your optimal value of lambda. This is how we choose the estimated best model with optimal hyper-parameter values. Use this same process with different types of algorithms like Ridge, LASSO, Elastic-Net, Random Forests, and Boosted trees."
      ],
      "metadata": {
        "id": "93NFUPWZwQ7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2gGAs1VwP-f"
      },
      "outputs": [],
      "source": []
    }
  ]
}